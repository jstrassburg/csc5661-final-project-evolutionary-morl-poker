{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec10883e",
   "metadata": {},
   "source": [
    "# Evolutionary Multi-Objective Reinforcement Learning for Texas Holdem\n",
    "\n",
    "In this notebook, I will:\n",
    "\n",
    "- Define an environment to represent the game of fixed-limit Texas Holdem\n",
    "- Create six agents:\n",
    "  - Four to represent the well-known poker player archetypes: \n",
    "    - Tight-aggressive (TAG) - A normally tight player who gets aggressive with quality hands.\n",
    "    - Loose-aggressive (LAG) - A player who aggressively plays lots of hands regardless of quality.\n",
    "    - Loose-passive (calling station) - A player who plays lots of pots but doesn't aggressively raise quality hands.\n",
    "    - Tight-passive (nit or rock) - A player who plays few hands and only commits chips with quality hands.\n",
    "  - And two to represent our RL trained agents.\n",
    "\n",
    "The RL trained agents will use an evolutionary (genetic) algorithm and evolutionary reinforcement learning techniques. One will be single objective (single policy to maximize winnings) and the other will be multi-objective (a different policy for each of the playing styles noted above).\n",
    "\n",
    "I will then compare the performance of the single-objective agent to the multi-objective agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450346a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Iterable\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from enum import Enum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from deap import algorithms, base, creator, tools\n",
    "from pokerkit import (Automation, Card, Deck, FixedLimitTexasHoldem, Rank,\n",
    "                      StandardHighHand, Suit, calculate_hand_strength,\n",
    "                      parse_range)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the available actions that a player can take\n",
    "# when it is their turn to act.\n",
    "class ActionType(Enum):\n",
    "    FOLD = 0\n",
    "    CHECK_OR_CALL = 1\n",
    "    BET_OR_RAISE = 2\n",
    "\n",
    "# The different streets in Texas Hold'em\n",
    "class StreetType(Enum):\n",
    "    PRE_FLOP = 0\n",
    "    FLOP = 1\n",
    "    TURN = 2\n",
    "    RIVER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f730eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerEquityCalculator:\n",
    "    \"\"\"\n",
    "    Utility class to calculate the strength of a given hand\n",
    "    on a given board using Monte Carlo simulations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._executor = ProcessPoolExecutor()\n",
    "        self._hand_strength_cache = {}\n",
    "\n",
    "    def calculate_strength(\n",
    "        self,\n",
    "        hand: str,\n",
    "        board: Iterable[Card],\n",
    "        players_left_in_hand: int = 5,\n",
    "        sample_count: int = 250,\n",
    "    ) -> float:\n",
    "        cache_key = (hand, tuple(board), players_left_in_hand, sample_count)\n",
    "        if cache_key in self._hand_strength_cache:\n",
    "            return self._hand_strength_cache[cache_key]\n",
    "        strength = calculate_hand_strength(\n",
    "            player_count=players_left_in_hand,\n",
    "            hole_range=parse_range(hand),\n",
    "            board_cards=board,\n",
    "            hole_dealing_count=2,\n",
    "            board_dealing_count=5,\n",
    "            deck=Deck.STANDARD,\n",
    "            hand_types=(StandardHighHand,),\n",
    "            sample_count=sample_count,\n",
    "            executor=self._executor,\n",
    "        )\n",
    "        self._hand_strength_cache[cache_key] = strength\n",
    "        return strength\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        if self._executor:\n",
    "            self._executor.shutdown(wait=True)\n",
    "            self._executor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservableState:\n",
    "    \"\"\"Represents the observable state for a player making game-time decisions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hand: Iterable[Iterable[Card]],  # The player's hole cards\n",
    "        board: Iterable[Card],  # Any cards on the board\n",
    "        can_fold: bool,  # If the player can fold\n",
    "        can_check_or_call: bool,  # If the player can check or call\n",
    "        can_complete_bet_or_raise_to: bool,  # If the player can bet or raise\n",
    "        players_left_in_hand: int,  # Number of players left in the hand\n",
    "        hand_strength: float,  # The strength of the player's hand based on who's left and the board\n",
    "        pot_size: int,  # Current size of the pot\n",
    "        checking_or_calling_amount: int,  # Amount needed to check or call\n",
    "        min_completion_betting_or_raising_to_amount: int,  # Minimum amount to bet or raise to\n",
    "        street: StreetType,  # The current street in the hand\n",
    "        next_to_act: int,  # The index of the next player to act\n",
    "    ) -> None:\n",
    "        self.hand: Iterable[Iterable[Card]] = hand\n",
    "        self.board: Iterable[Card] = board\n",
    "        self.can_fold: bool = can_fold\n",
    "        self.can_check_or_call: bool = can_check_or_call\n",
    "        self.can_complete_bet_or_raise_to: bool = can_complete_bet_or_raise_to\n",
    "        self.players_left_in_hand: int = players_left_in_hand\n",
    "        self.hand_strength: float = hand_strength\n",
    "        self.pot_size: int = pot_size\n",
    "        self.checking_or_calling_amount: int = checking_or_calling_amount\n",
    "        self.min_completion_betting_or_raising_to_amount: int = (\n",
    "            min_completion_betting_or_raising_to_amount\n",
    "        )\n",
    "        self.street: StreetType = street\n",
    "        self.next_to_act: int = next_to_act\n",
    "\n",
    "        # calculate pot odds\n",
    "        self.pot_odds_check_or_call = checking_or_calling_amount / (\n",
    "            pot_size + checking_or_calling_amount\n",
    "        ) if can_check_or_call else 1.0\n",
    "\n",
    "        self.pot_odds_bet_or_raise = min_completion_betting_or_raising_to_amount / (\n",
    "            pot_size + min_completion_betting_or_raising_to_amount\n",
    "        ) if can_complete_bet_or_raise_to else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417caa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerAgent(ABC):\n",
    "    \"\"\"A abstract player class. It's policy will be defined per-implementation.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def π(self, observation: ObservableState) -> ActionType:\n",
    "        pass\n",
    "\n",
    "    def _compute_action(\n",
    "        self,\n",
    "        observation: ObservableState,\n",
    "        strength_to_bet_or_raise_pre_flop: float,\n",
    "        strength_to_check_or_call_pre_flop: float,\n",
    "        strength_over_pot_odds_to_bet_or_raise_post_flop: float,\n",
    "        strength_over_pot_odds_to_check_or_call_post_flop: float,\n",
    "    ) -> ActionType:\n",
    "        hand_strength = observation.hand_strength\n",
    "        pot_odds_check_or_call = observation.pot_odds_check_or_call\n",
    "        pot_odds_bet_or_raise = observation.pot_odds_bet_or_raise\n",
    "\n",
    "        if observation.street == StreetType.PRE_FLOP:\n",
    "            if (\n",
    "                hand_strength > strength_to_bet_or_raise_pre_flop\n",
    "                and observation.can_complete_bet_or_raise_to\n",
    "            ):\n",
    "                return ActionType.BET_OR_RAISE\n",
    "            elif (\n",
    "                hand_strength > strength_to_check_or_call_pre_flop\n",
    "                or observation.checking_or_calling_amount == 0\n",
    "                and observation.can_check_or_call\n",
    "            ):\n",
    "                return ActionType.CHECK_OR_CALL\n",
    "            else:\n",
    "                return ActionType.FOLD\n",
    "        else:\n",
    "            if (\n",
    "                hand_strength - pot_odds_bet_or_raise\n",
    "                > strength_over_pot_odds_to_bet_or_raise_post_flop\n",
    "                and observation.can_complete_bet_or_raise_to\n",
    "            ):\n",
    "                return ActionType.BET_OR_RAISE\n",
    "            elif (\n",
    "                hand_strength - pot_odds_check_or_call\n",
    "                > strength_over_pot_odds_to_check_or_call_post_flop\n",
    "                or observation.checking_or_calling_amount == 0\n",
    "                and observation.can_check_or_call\n",
    "            ):\n",
    "                return ActionType.CHECK_OR_CALL\n",
    "            else:\n",
    "                return ActionType.FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b991a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerPlayer:\n",
    "    \"\"\"The information needed to represent a player in the poker environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, poker_agent: PokerAgent, player_index: int, stack: int = 100000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a PokerPlayer.\n",
    "        Args:\n",
    "            poker_agent (PokerAgent): The agent controlling this player's decisions/policy.\n",
    "            player_index (int): The index of the player at the table. Needed for button rotation.\n",
    "            stack (int): The player's starting stack size.\n",
    "        \"\"\"\n",
    "        self.poker_agent = poker_agent\n",
    "        self.player_index = player_index\n",
    "        self.stack = stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedLimitTexasHoldemEnvironment:\n",
    "    \"\"\"An environment for Fixed Limit Texas Hold'em Poker\"\"\"\n",
    "\n",
    "    def __init__(self, players: list[PokerPlayer]) -> None:\n",
    "        self._pokers_equity_calculator = PokerEquityCalculator()\n",
    "        self._players = players\n",
    "        starting_stacks = [player.stack for player in self._players]\n",
    "\n",
    "        self._game = FixedLimitTexasHoldem(\n",
    "            # We're running a simulation here so no need for people to actually\n",
    "            # take these actions. These are automated by the engine.\n",
    "            automations=(\n",
    "                Automation.ANTE_POSTING,\n",
    "                Automation.BET_COLLECTION,\n",
    "                Automation.BLIND_OR_STRADDLE_POSTING,\n",
    "                Automation.CARD_BURNING,\n",
    "                Automation.HOLE_DEALING,\n",
    "                Automation.BOARD_DEALING,\n",
    "                Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n",
    "                Automation.HAND_KILLING,\n",
    "                Automation.CHIPS_PUSHING,\n",
    "                Automation.CHIPS_PULLING,\n",
    "            ),\n",
    "            ante_trimming_status=True,  # use blinds\n",
    "            raw_antes=0,\n",
    "            raw_blinds_or_straddles=(2, 4),\n",
    "            small_bet=4,\n",
    "            big_bet=8,\n",
    "            starting_board_count=1,\n",
    "        )\n",
    "\n",
    "        self._state = self._game(\n",
    "            raw_starting_stacks=starting_stacks,\n",
    "            player_count=len(self._players),\n",
    "        )\n",
    "\n",
    "        self._hand_strength_cache = {}\n",
    "\n",
    "    def get_next_to_act_player_index(self) -> int:\n",
    "        next_to_act = self._state.actor_indices[0]\n",
    "        return self._players[next_to_act].player_index\n",
    "\n",
    "    def _calculate_hand_strength(self) -> float:\n",
    "        next_to_act = (\n",
    "            self._state.actor_indices[0] if len(self._state.actor_indices) > 0 else None\n",
    "        )\n",
    "        hole_cards = (\n",
    "            self._state.hole_cards[next_to_act] if next_to_act is not None else []\n",
    "        )\n",
    "        hand = \"\".join([f\"{str(card.rank)}{str(card.suit)}\" for card in hole_cards])\n",
    "        board_cards_flat = (\n",
    "            [card[0] for card in self._state.board_cards]\n",
    "            if self._state.board_cards\n",
    "            else []\n",
    "        )\n",
    "        players_left_in_hand = sum(1 for status in self._state.statuses if status)\n",
    "        return self._pokers_equity_calculator.calculate_strength(\n",
    "            hand=hand,\n",
    "            board=board_cards_flat,\n",
    "            players_left_in_hand=players_left_in_hand,\n",
    "        )\n",
    "\n",
    "    def reset(self) -> ObservableState:\n",
    "        \"\"\"\n",
    "        Resets the environment to start a new hand with the same players\n",
    "        Returns the initial observable state.\n",
    "        \"\"\"\n",
    "        # Save the old state. Unsure yet if I will use this.\n",
    "        self._old_state = copy.deepcopy(self._state)\n",
    "\n",
    "        # Rotate players for the next hand to reflect the dealer button moving\n",
    "        # around the table.\n",
    "        self._players = list(self._players[1:]) + [self._players[0]]\n",
    "\n",
    "        # Remove any bankrupt players though I will probably\n",
    "        # make the initial stacks very high to avoid this complication.\n",
    "        for i, player in enumerate(self._players):\n",
    "            if player.stack == 0:\n",
    "                del self._players[i]\n",
    "\n",
    "        current_stacks = [player.stack for player in self._players]\n",
    "\n",
    "        # Reset the game state for a new hand\n",
    "        self._state = self._game(\n",
    "            raw_starting_stacks=current_stacks,\n",
    "            player_count=len(self._players),\n",
    "        )\n",
    "\n",
    "        next_to_act = self._state.actor_indices[0]\n",
    "        hand_strength = self._calculate_hand_strength()\n",
    "        players_left_in_hand = sum(1 for status in self._state.statuses if status)\n",
    "        return ObservableState(\n",
    "            hand=self._state.hole_cards[next_to_act],\n",
    "            board=self._state.board_cards,\n",
    "            can_fold=self._state.can_fold(),\n",
    "            can_check_or_call=self._state.can_check_or_call(),\n",
    "            can_complete_bet_or_raise_to=self._state.can_complete_bet_or_raise_to(),\n",
    "            players_left_in_hand=players_left_in_hand,\n",
    "            hand_strength=hand_strength,\n",
    "            pot_size=self._state.total_pot_amount,\n",
    "            checking_or_calling_amount=self._state.checking_or_calling_amount,\n",
    "            min_completion_betting_or_raising_to_amount=self._state.min_completion_betting_or_raising_to_amount,\n",
    "            street=StreetType.PRE_FLOP,\n",
    "            next_to_act=self.get_next_to_act_player_index(),\n",
    "        )\n",
    "\n",
    "    def action_space(self) -> list[ActionType]:\n",
    "        \"\"\"Returns the action space (what the player can do right now) for the environment.\"\"\"\n",
    "        available_actions = []\n",
    "        if self._state.can_fold():\n",
    "            available_actions.append(ActionType.FOLD)\n",
    "        if self._state.can_check_or_call():\n",
    "            available_actions.append(ActionType.CHECK_OR_CALL)\n",
    "        if self._state.can_bet_or_raise():\n",
    "            available_actions.append(ActionType.BET_OR_RAISE)\n",
    "        return available_actions\n",
    "\n",
    "    def step(self, action: ActionType) -> dict:\n",
    "        \"\"\"\n",
    "        Takes a step in the environment based on the action and returns:\n",
    "            - state: the new state of the environment\n",
    "            - reward: the reward obtained from taking the action\n",
    "            - done: whether the hand is over\n",
    "        \"\"\"\n",
    "        board_len = len(self._state.board_cards)\n",
    "        street = StreetType.PRE_FLOP\n",
    "        if board_len == 3:\n",
    "            street = StreetType.FLOP\n",
    "        elif board_len == 4:\n",
    "            street = StreetType.TURN\n",
    "        elif board_len == 5:\n",
    "            street = StreetType.RIVER\n",
    "\n",
    "        # Take the action\n",
    "        if action == ActionType.FOLD:\n",
    "            self._state.fold()\n",
    "        elif action == ActionType.CHECK_OR_CALL:\n",
    "            self._state.check_or_call()\n",
    "        elif action == ActionType.BET_OR_RAISE:\n",
    "            self._state.complete_bet_or_raise_to()\n",
    "\n",
    "        done = not self._state.status\n",
    "        rewards = None\n",
    "        if done:\n",
    "            # update player stacks\n",
    "            for i, player in enumerate(self._players):\n",
    "                player.stack = self._state.stacks[i]\n",
    "            # gather rewards\n",
    "            payoffs = self._state.payoffs\n",
    "            rewards = [0] * len(self._players)\n",
    "            # Unwind the shifting of the players to account\n",
    "            # for the dealer button moving.\n",
    "            for i, player in enumerate(self._players):\n",
    "                rewards[player.player_index] = payoffs[i]\n",
    "\n",
    "        self._old_state = copy.deepcopy(self._state)\n",
    "\n",
    "        next_to_act = (\n",
    "            self._state.actor_indices[0] if self._state.actor_indices else None\n",
    "        )\n",
    "        hand_strength = self._calculate_hand_strength() if not done else 0.0\n",
    "        players_left_in_hand = sum(1 for status in self._state.statuses if status)\n",
    "        observation = ObservableState(\n",
    "            hand=self._state.hole_cards[next_to_act] if not done else [],\n",
    "            board=self._state.board_cards if not done else [],\n",
    "            can_fold=self._state.can_fold(),\n",
    "            can_check_or_call=self._state.can_check_or_call(),\n",
    "            can_complete_bet_or_raise_to=self._state.can_complete_bet_or_raise_to(),\n",
    "            players_left_in_hand=players_left_in_hand,\n",
    "            hand_strength=hand_strength,\n",
    "            pot_size=self._state.total_pot_amount,\n",
    "            checking_or_calling_amount=self._state.checking_or_calling_amount,\n",
    "            min_completion_betting_or_raising_to_amount=self._state.min_completion_betting_or_raising_to_amount,\n",
    "            street=street,\n",
    "            next_to_act=(\n",
    "                self.get_next_to_act_player_index()\n",
    "                if self._state.actor_indices\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"observation\": observation,\n",
    "            \"done\": done,\n",
    "            \"rewards\": rewards,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fbeea",
   "metadata": {},
   "source": [
    "## Player Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de535e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentRandom(PokerAgent):\n",
    "    \"\"\"A poker agent that takes random actions from the available action space.\"\"\"\n",
    "\n",
    "    def π(self, observation: ObservableState, **kwargs) -> ActionType:\n",
    "        available_actions = []\n",
    "        if observation.can_fold:\n",
    "            available_actions.append(ActionType.FOLD)\n",
    "        if observation.can_check_or_call:\n",
    "            available_actions.append(ActionType.CHECK_OR_CALL)\n",
    "        if observation.can_complete_bet_or_raise_to:\n",
    "            available_actions.append(ActionType.BET_OR_RAISE)\n",
    "        return np.random.choice(available_actions)\n",
    "\n",
    "\n",
    "class AgentTightAggressive(PokerAgent):\n",
    "    \"\"\"\n",
    "    A poker agent that plays a tight-aggressive style. A TAG.\n",
    "    Requires quality hands to raise. Plays fewer pots.\n",
    "    \"\"\"\n",
    "\n",
    "    def π(self, observation: ObservableState, **kwargs) -> ActionType:\n",
    "        return self._compute_action(\n",
    "            observation,\n",
    "            strength_to_bet_or_raise_pre_flop=0.6,\n",
    "            strength_to_check_or_call_pre_flop=0.4,\n",
    "            strength_over_pot_odds_to_bet_or_raise_post_flop=0.1,\n",
    "            strength_over_pot_odds_to_check_or_call_post_flop=0.0,\n",
    "        )\n",
    "\n",
    "class AgentLooseAggressive(PokerAgent):\n",
    "    \"\"\"\n",
    "    A poker agent that plays a loose-aggressive style. A LAG.\n",
    "    Doesn't need much to raise. Plays lots of pots.\n",
    "    \"\"\"\n",
    "\n",
    "    def π(self, observation: ObservableState, **kwargs) -> ActionType:\n",
    "        return self._compute_action(\n",
    "            observation,\n",
    "            strength_to_bet_or_raise_pre_flop=0.35,\n",
    "            strength_to_check_or_call_pre_flop=0.25,\n",
    "            strength_over_pot_odds_to_bet_or_raise_post_flop=-0.05,\n",
    "            strength_over_pot_odds_to_check_or_call_post_flop=0.05,\n",
    "        )\n",
    "    \n",
    "\n",
    "class AgentTightPassive(PokerAgent):\n",
    "    \"\"\"\n",
    "    A poker agent that plays a tight-passive style. A rock.\n",
    "    Requires very strong hands to raise or bet. Plays fewer pots.\n",
    "    \"\"\"\n",
    "\n",
    "    def π(self, observation: ObservableState, **kwargs) -> ActionType:\n",
    "        return self._compute_action(\n",
    "            observation,\n",
    "            strength_to_bet_or_raise_pre_flop=0.75,\n",
    "            strength_to_check_or_call_pre_flop=0.55,\n",
    "            strength_over_pot_odds_to_bet_or_raise_post_flop=0.25,\n",
    "            strength_over_pot_odds_to_check_or_call_post_flop=0.15,\n",
    "        )\n",
    "    \n",
    "class AgentLoosePassive(PokerAgent):\n",
    "    \"\"\"\n",
    "    A poker agent that plays a loose-passive style. A calling station.\n",
    "    Prefers calling to raising. Plays lots of pots.\n",
    "    \"\"\"\n",
    "\n",
    "    def π(self, observation: ObservableState, **kwargs) -> ActionType:\n",
    "        return self._compute_action(\n",
    "            observation,\n",
    "            strength_to_bet_or_raise_pre_flop=0.75,\n",
    "            strength_to_check_or_call_pre_flop=0.25,\n",
    "            strength_over_pot_odds_to_bet_or_raise_post_flop=0.50,\n",
    "            strength_over_pot_odds_to_check_or_call_post_flop=0.25,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d151d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of the coded agents rules\n",
    "observation = ObservableState(\n",
    "    hand=[[Card(Rank.ACE, Suit.SPADE), Card(Rank.ACE, Suit.HEART)]],\n",
    "    board=[\n",
    "        Card(Rank.KING, Suit.DIAMOND),\n",
    "        Card(Rank.QUEEN, Suit.SPADE),\n",
    "        Card(Rank.JACK, Suit.CLUB),\n",
    "    ],\n",
    "    can_fold=True,\n",
    "    can_check_or_call=True,\n",
    "    can_complete_bet_or_raise_to=True,\n",
    "    players_left_in_hand=4,\n",
    "    hand_strength=0.3,\n",
    "    pot_size=12,\n",
    "    checking_or_calling_amount=4,\n",
    "    min_completion_betting_or_raising_to_amount=4,\n",
    "    street=StreetType.FLOP,\n",
    "    next_to_act=0,\n",
    ")\n",
    "\n",
    "print(\"Sanity check of the agent configurations:\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "tag = AgentTightAggressive()\n",
    "action = tag.π(observation)\n",
    "print(f\"Selected tight aggressive action:\\t{action}\")\n",
    "\n",
    "rock = AgentTightPassive()\n",
    "action = rock.π(observation)\n",
    "print(f\"Selected tight passive action:\\t\\t{action}\")\n",
    "\n",
    "lag = AgentLooseAggressive()\n",
    "action = lag.π(observation)\n",
    "print(f\"Selected loose aggressive action:\\t{action}\")\n",
    "\n",
    "calling_station = AgentLoosePassive()\n",
    "action = calling_station.π(observation)\n",
    "print(f\"Selected loose passive action:\\t\\t{action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85604539",
   "metadata": {},
   "source": [
    "## Playing Poker\n",
    "\n",
    "The following method is used to complete 1 episode (hand of poker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_hand_of_poker(\n",
    "    env: FixedLimitTexasHoldemEnvironment,\n",
    "    players: list[PokerPlayer],\n",
    "    actions_taken: list[list[ActionType]],\n",
    "    θ: np.ndarray = None,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Claude 4.5: I tab completed the docstring.\n",
    "    Play one hand of poker (episode) in the provided environment.\n",
    "    Args:\n",
    "        env (FixedLimitTexasHoldemEnvironment): The poker environment.\n",
    "        players (list[PokerPlayer]): The players in the environment.\n",
    "        actions_taken (list[list[ActionType]]): A list to record the actions taken by\n",
    "            each player for graphing later.\n",
    "        θ (np.ndarray, optional): Supplied when training an evolutionary agent. Ignored otherwise.\n",
    "    Returns:\n",
    "        list[int]: The rewards obtained by each player at the end of the hand.\n",
    "    \"\"\"\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        next_to_act_index = observation.next_to_act\n",
    "        next_to_act = players[next_to_act_index]\n",
    "\n",
    "        # Get the action from the agent's policy function π\n",
    "        action = next_to_act.poker_agent.π(observation, θ=θ)\n",
    "\n",
    "        # Save the actions taken for graphing later\n",
    "        actions_taken[next_to_act_index].append(action)\n",
    "\n",
    "        # Take a step\n",
    "        step_result = env.step(action)\n",
    "\n",
    "        observation = step_result[\"observation\"]\n",
    "        done = step_result[\"done\"]\n",
    "        if done:\n",
    "            # We only have rewards at the end of the hand\n",
    "            rewards = step_result[\"rewards\"]\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdcce60",
   "metadata": {},
   "source": [
    "## Random Agent Baseline\n",
    "\n",
    "In the following section, I will let the random agent compete against the coded agents to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_stack = 10000  # Start high to avoid going bankrupt\n",
    "player_0 = PokerPlayer(poker_agent=AgentRandom(), player_index=0, stack=starting_stack)\n",
    "player_1 = PokerPlayer(\n",
    "    poker_agent=AgentTightAggressive(), player_index=1, stack=starting_stack\n",
    ")\n",
    "player_2 = PokerPlayer(\n",
    "    poker_agent=AgentLooseAggressive(), player_index=2, stack=starting_stack\n",
    ")\n",
    "player_3 = PokerPlayer(\n",
    "    poker_agent=AgentTightPassive(), player_index=3, stack=starting_stack\n",
    ")\n",
    "player_4 = PokerPlayer(\n",
    "    poker_agent=AgentLoosePassive(), player_index=4, stack=starting_stack\n",
    ")\n",
    "players = [player_0, player_1, player_2, player_3, player_4]\n",
    "actions_taken = [[] for _ in players]\n",
    "rewards_over_time = [[] for _ in players]\n",
    "\n",
    "env = FixedLimitTexasHoldemEnvironment(players=players)\n",
    "num_hands = 100\n",
    "\n",
    "for hand in tqdm(\n",
    "    range(num_hands), desc=\"Playing hands with random agent vs. coded agents.\"\n",
    "):\n",
    "    rewards = play_one_hand_of_poker(\n",
    "        env=env,\n",
    "        players=players,\n",
    "        actions_taken=actions_taken,\n",
    "    )\n",
    "\n",
    "    # Save the rewards for graphing later\n",
    "    for i, reward in enumerate(rewards):\n",
    "        rewards_over_time[i].append(reward)\n",
    "\n",
    "    print(f\"Completed hand {hand}.\")\n",
    "    print(f\"Player stacks: {[player.stack for player in players]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e640400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.5 prompt: plot the actions taken by the agents in a stacked bar chart\n",
    "# Create a stacked bar chart showing action distribution for each player\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Calculate action counts for each player\n",
    "fold_counts = []\n",
    "call_counts = []\n",
    "raise_counts = []\n",
    "\n",
    "for actions in actions_taken:\n",
    "    fold_counts.append(sum(1 for a in actions if a == ActionType.FOLD))\n",
    "    call_counts.append(sum(1 for a in actions if a == ActionType.CHECK_OR_CALL))\n",
    "    raise_counts.append(sum(1 for a in actions if a == ActionType.BET_OR_RAISE))\n",
    "\n",
    "# Create the stacked bar chart\n",
    "player_names = [\n",
    "    \"Random\",\n",
    "    \"Tight Aggressive\\n(TAG)\",\n",
    "    \"Loose Aggressive\\n(LAG)\",\n",
    "    \"Tight Passive\\n(Rock)\",\n",
    "    \"Loose Passive\\n(Calling Station)\",\n",
    "]\n",
    "x = np.arange(len(player_names))\n",
    "width = 0.6\n",
    "\n",
    "bars1 = ax.bar(x, fold_counts, width, label=\"Fold\", color=\"#d62728\")\n",
    "bars2 = ax.bar(\n",
    "    x, call_counts, width, bottom=fold_counts, label=\"Check/Call\", color=\"#2ca02c\"\n",
    ")\n",
    "bars3 = ax.bar(\n",
    "    x,\n",
    "    raise_counts,\n",
    "    width,\n",
    "    bottom=np.array(fold_counts) + np.array(call_counts),\n",
    "    label=\"Bet/Raise\",\n",
    "    color=\"#1f77b4\",\n",
    ")\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel(\"Player Agent\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Number of Actions\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Action Distribution by Player Agent\\n{num_hands} Hands\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(player_names)\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add percentage labels on each segment\n",
    "for i in range(len(player_names)):\n",
    "    total_actions = fold_counts[i] + call_counts[i] + raise_counts[i]\n",
    "\n",
    "    # Fold percentage\n",
    "    if fold_counts[i] > 0:\n",
    "        ax.text(\n",
    "            i,\n",
    "            fold_counts[i] / 2,\n",
    "            f\"{fold_counts[i]/total_actions*100:.1f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "            color=\"white\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Call percentage\n",
    "    if call_counts[i] > 0:\n",
    "        ax.text(\n",
    "            i,\n",
    "            fold_counts[i] + call_counts[i] / 2,\n",
    "            f\"{call_counts[i]/total_actions*100:.1f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "            color=\"white\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Raise percentage\n",
    "    if raise_counts[i] > 0:\n",
    "        ax.text(\n",
    "            i,\n",
    "            fold_counts[i] + call_counts[i] + raise_counts[i] / 2,\n",
    "            f\"{raise_counts[i]/total_actions*100:.1f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "            color=\"white\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.5 prompt: Visualize cumulative profit/loss over time for each player\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Calculate cumulative rewards for each player\n",
    "cumulative_rewards = []\n",
    "for player_rewards in rewards_over_time:\n",
    "    cumulative = np.cumsum(player_rewards)\n",
    "    cumulative_rewards.append(cumulative)\n",
    "\n",
    "# Plot cumulative rewards for each player\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\"]\n",
    "for i, (player_name, cumulative_reward, color) in enumerate(\n",
    "    zip(player_names, cumulative_rewards, colors)\n",
    "):\n",
    "    ax.plot(\n",
    "        range(len(cumulative_reward)),\n",
    "        cumulative_reward,\n",
    "        label=player_name,\n",
    "        color=color,\n",
    "        linewidth=2.5,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "# Add horizontal line at y=0 to show break-even\n",
    "ax.axhline(\n",
    "    y=0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.5, label=\"Break Even\"\n",
    ")\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel(\"Hand Number\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Cumulative Profit/Loss (chips)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Player Performance Over {num_hands} Hands\\n(Starting Stack: {starting_stack} chips)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.legend(loc=\"best\", fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add final profit/loss annotation for each player\n",
    "for i, (player_name, cumulative_reward, color) in enumerate(\n",
    "    zip(player_names, cumulative_rewards, colors)\n",
    "):\n",
    "    final_profit = cumulative_reward[-1]\n",
    "    ax.annotate(\n",
    "        f\"{final_profit:+d}\",\n",
    "        xy=(len(cumulative_reward) - 1, final_profit),\n",
    "        xytext=(10, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=9,\n",
    "        fontweight=\"bold\",\n",
    "        color=color,\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round,pad=0.3\", facecolor=\"white\", edgecolor=color, alpha=0.8\n",
    "        ),\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Starting stack: {starting_stack:,} chips\\n\")\n",
    "for i, (player_name, player) in enumerate(zip(player_names, players)):\n",
    "    final_stack = player.stack\n",
    "    profit_loss = final_stack - starting_stack\n",
    "    profit_pct = (profit_loss / starting_stack) * 100\n",
    "    print(\n",
    "        f\"{player_name:<30} | Final: {final_stack:,} | P/L: {profit_loss:+,} ({profit_pct:+.2f}%)\"\n",
    "    )\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07e437",
   "metadata": {},
   "source": [
    "## Evolutionary Agent Base Class\n",
    "\n",
    "This class will be inherited by both our single- and multi-objective evolutionary agents. It contains shared methods for encoding the observation vector (φ), decoding our parameter vector (θ), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionaryPokerAgent(PokerAgent):\n",
    "    \"\"\"\n",
    "    A abstract evolutionary agent class to handle the encoding/decoding\n",
    "    of the parameter vector θ which is also our chromosome for learning.\n",
    "\n",
    "    This base class contains the method for encoding the observation vector,\n",
    "    φ, a method to decode θ into weights and biases, and a simple softmax\n",
    "    implementation (which I know is in scipy but it is simple enough to avoid the\n",
    "    extra dependency).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # hand_strength, pot_size, checking_or_calling_amount,\n",
    "        # min_completion_betting_or_raising_to_amount,\n",
    "        # street (one-hot encoded): is_pre_flop, is_flop, is_turn, is_river\n",
    "        self.N_FEATURES = 8\n",
    "\n",
    "        # fold, check/call, bet/raise\n",
    "        self.N_ACTIONS = 3\n",
    "\n",
    "    def encode_observation_vector(self, observation: ObservableState) -> np.ndarray:\n",
    "        \"\"\"Encodes the observation into a vector (φ).\"\"\"\n",
    "        # one-hot encode the street\n",
    "        encoded_street = np.zeros(4)\n",
    "        encoded_street[observation.street.value] = 1\n",
    "\n",
    "        # Handle None values from pokerkit - set to 0 when action is unavailable\n",
    "        checking_or_calling_amount = (\n",
    "            observation.checking_or_calling_amount\n",
    "            if observation.checking_or_calling_amount is not None\n",
    "            else 0\n",
    "        )\n",
    "        min_completion_betting_or_raising_to_amount = (\n",
    "            observation.min_completion_betting_or_raising_to_amount\n",
    "            if observation.min_completion_betting_or_raising_to_amount is not None\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        φ = np.array(\n",
    "            [\n",
    "                observation.hand_strength,\n",
    "                observation.pot_size,\n",
    "                checking_or_calling_amount,\n",
    "                min_completion_betting_or_raising_to_amount,\n",
    "                *encoded_street,\n",
    "            ]\n",
    "        )\n",
    "        return φ\n",
    "\n",
    "    def decode_theta(self, θ: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"ChatGPT 5.1 prompt: Decode theta for N_FEATURES and N_ACTIONS.\"\"\"\n",
    "        θ = np.asarray(θ, dtype=float)\n",
    "        W = θ[: self.N_FEATURES * self.N_ACTIONS].reshape(\n",
    "            self.N_ACTIONS, self.N_FEATURES\n",
    "        )\n",
    "        b = θ[self.N_FEATURES * self.N_ACTIONS :]\n",
    "        return W, b\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "        # Claude 4.5: tab-complete after typing the method signature\n",
    "        exp_logits = np.exp(logits - np.max(logits))  # for numerical stability\n",
    "        return exp_logits / exp_logits.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba31c1",
   "metadata": {},
   "source": [
    "## Single-Objective Evolutionary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEvolutionarySingleObjective(EvolutionaryPokerAgent):\n",
    "    \"\"\"\n",
    "    A poker agent that uses a single-objective evolutionary algorithm\n",
    "    to learn its _sole_ policy's parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def π(self, observation: ObservableState, θ: np.ndarray, **kwargs) -> ActionType:\n",
    "        if θ is None:\n",
    "            raise ValueError(\"θ must be provided for the evolutionary agent.\")\n",
    "        W, b = self.decode_theta(θ)\n",
    "        φ = self.encode_observation_vector(observation)\n",
    "        logits = W @ φ + b\n",
    "        action_probs = self.softmax(logits)\n",
    "        action_index = np.random.choice(range(len(action_probs)), p=action_probs)\n",
    "        action = ActionType(action_index)\n",
    "        if (\n",
    "            action == ActionType.FOLD\n",
    "            and observation.can_check_or_call\n",
    "            and observation.checking_or_calling_amount == 0\n",
    "        ):\n",
    "            action = ActionType.CHECK_OR_CALL # avoid folding when we can check for free\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55c2b9",
   "metadata": {},
   "source": [
    "## Configure Environment for Evolutionary Agent\n",
    "\n",
    "The following environment is configured with the single-objective evolutionary agent against the four coded playing types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791dc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_0 = PokerPlayer(\n",
    "    poker_agent=AgentEvolutionarySingleObjective(), player_index=0, stack=starting_stack\n",
    ")\n",
    "player_1 = PokerPlayer(\n",
    "    poker_agent=AgentTightAggressive(), player_index=1, stack=starting_stack\n",
    ")\n",
    "player_2 = PokerPlayer(\n",
    "    poker_agent=AgentLooseAggressive(), player_index=2, stack=starting_stack\n",
    ")\n",
    "player_3 = PokerPlayer(\n",
    "    poker_agent=AgentTightPassive(), player_index=3, stack=starting_stack\n",
    ")\n",
    "player_4 = PokerPlayer(\n",
    "    poker_agent=AgentLoosePassive(), player_index=4, stack=starting_stack\n",
    ")\n",
    "players = [player_0, player_1, player_2, player_3, player_4]\n",
    "\n",
    "single_objective_env = FixedLimitTexasHoldemEnvironment(players=players)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458a428",
   "metadata": {},
   "source": [
    "## Single-Objective Fitness Function\n",
    "\n",
    "The following fitness function will be used to evaluate our parameter vector (θ) by playing several hands of poker and returning the average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_single_objective(\n",
    "    θ: np.ndarray,\n",
    ") -> tuple[float]:\n",
    "    \"\"\"\n",
    "    Claude 4.5 prompt: I tab completed the docstring after typing the method signature.\n",
    "    Fitness function for single-objective evolutionary poker agent.\n",
    "    Plays multiple hands and returns average profit as fitness.\n",
    "    Args:\n",
    "        θ (np.ndarray): The parameter vector for the evolutionary agent.\n",
    "    Returns:\n",
    "        tuple[float]: The average profit as fitness.\n",
    "    \"\"\"\n",
    "    num_hands = 5\n",
    "    total_reward = 0.0\n",
    "    for _ in tqdm(range(num_hands), desc=\"Evaluating fitness...\"):\n",
    "        rewards = play_one_hand_of_poker(\n",
    "            env=single_objective_env,\n",
    "            players=players,\n",
    "            actions_taken=[[] for _ in players],  # No need to track actions here\n",
    "            θ=θ,\n",
    "        )\n",
    "        total_reward += rewards[0]  # Reward for the evolutionary agent\n",
    "    average_reward = total_reward / num_hands\n",
    "    return (average_reward,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea57c21",
   "metadata": {},
   "source": [
    "## Configure Evolutionary Algorithm for Single-Objective Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a48da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize fitness\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# register a tool to generate candidate solutions\n",
    "toolbox.register(\"attr_float\", random.gauss, 0.0, 1.0)\n",
    "\n",
    "# Should be 8 * 3 + 3 = 27 for our chromosome size\n",
    "chromosome_size = (\n",
    "    player_0.poker_agent.N_FEATURES * player_0.poker_agent.N_ACTIONS\n",
    "    + player_0.poker_agent.N_ACTIONS\n",
    ")\n",
    "\n",
    "toolbox.register(\n",
    "    \"individual\",\n",
    "    tools.initRepeat,\n",
    "    creator.Individual,\n",
    "    toolbox.attr_float,\n",
    "    n=chromosome_size,\n",
    ")\n",
    "\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# register our evaluation/fitness function\n",
    "toolbox.register(\"evaluate\", fitness_single_objective)\n",
    "\n",
    "# register our crossover/mating operator\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "\n",
    "# define the mutation parameters and register the mutation operator\n",
    "sigma = 0.1  # standard deviation for the Gaussian mutation\n",
    "mutation_probability = 0.2  # probability of mutation\n",
    "\n",
    "toolbox.register(\n",
    "    \"mutate\",\n",
    "    tools.mutGaussian,  # Gaussian mutation adds noise from the Gaussian/normal distribution\n",
    "    mu=0.0,\n",
    "    sigma=sigma,\n",
    "    indpb=mutation_probability,\n",
    ")\n",
    "\n",
    "# use a tournament to select candidates for mating\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 100\n",
    "NUM_GENERATIONS = 50\n",
    "CROSSOVER_PROBABILITY = 0.5\n",
    "MUTATION_PROBABILITY = 0.3\n",
    "\n",
    "# Initialize the population\n",
    "pop = toolbox.population(n=POPULATION_SIZE)\n",
    "\n",
    "# Configure statistics\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values[0])\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "# Keep track of the single best individual\n",
    "hof = tools.HallOfFame(1)\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "pop, logbook = algorithms.eaSimple(\n",
    "    population=pop,\n",
    "    toolbox=toolbox,\n",
    "    cxpb=CROSSOVER_PROBABILITY,\n",
    "    mutpb=MUTATION_PROBABILITY,\n",
    "    ngen=NUM_GENERATIONS,\n",
    "    stats=stats,\n",
    "    halloffame=hof,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Collect our best performing individual\n",
    "best_theta = np.array(hof[0], dtype=float)\n",
    "print(\"Best fitness:\", hof[0].fitness.values[0])\n",
    "print(\"Best theta:\", best_theta)\n",
    "\n",
    "best_theta, pop, logbook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc5661-final-project-evolutionary-morl-poker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
